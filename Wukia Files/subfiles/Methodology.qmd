# Methodology

## Data Collection

The data used in this study reside in the text archives of the Packard Humanities Institute (PHI) website. The archives contain a collection of Greek inscriptions as published in 746 books, journals, and epigraphic collections. One may find the PHI website [here](https://packhum.org/) (packhum.org) and the Greek inscriptional archive [here](http://inscriptions.packhum.org/) (inscriptions.packhum.org).

The Institute has organized its collection by region, containing inscriptions from Attica (IG I-III), the Peloponnesos (IG IV-\[VI\]), Central Greece (IG VII-IX), and beyond. Navigating through the page from Regions \> Attica (IG I-III) \> IG I^2^ will present a list of inscriptions found in Attica and published in *Inscriptiones Graecae* (IG) volume one part two. Selecting the first entry, IG I^2^ 165, will present the inscription's contents. The header rests at the top of the page, with the main text below it.

Each inscription also has a unique identifier, which the author calls the PH number. This number appears at the bottom right of the page when viewing a text. Note that viewing an inscription's URL will reveal that the Institute has used the PH numbers as the primary means of accessing an inscription. For example, IG I^2^ 165 has the unique identifier [PH1754](https://inscriptions.packhum.org/text/1754). Clicking on this identifier will open the same inscription, but with a different URL. Compare the URL retrieved by navigating through the menu tree (1) to that retrieved by clicking on the PH number (2) below.

1.  [`https://inscriptions.packhum.org/text/1754?&bookid=3&location=1701`](https://inscriptions.packhum.org/text/1754?&bookid=3&location=1701)

2.  [`https://inscriptions.packhum.org/text/1754`](https://inscriptions.packhum.org/text/1754)

The first URL contains extraneous information at the end, mainly a tag called `bookid` and one called `location`, each with selections following the assignment operator `=`. However, the two URLs match perfectly until the extra identifying tags, and both end with that inscription's PH number. Replacing the number at the end of the URL with any other valid PH number will return a different inscription, opening the door to automation and allowing one to rapidly collect large volumes of inscriptional data.

To gather the data necessary for the study, the author has composed a web scraper: a file of computer code which systematically retrieves data from an online source. The author has elected to write this program in a language called R, which the R Foundation for Statistical Computing developed and maintains. As the name implies, the Foundation designed R as a system for quick and high-powered statistical analyses, resulting in a language with built-in tools for data management and testing which one might otherwise need to compose or load separately for other languages. As such, R has become a natural complement for linguists integrating large data sets or quantitative methods into their research.

The web scraper contains fifteen functions --- discrete operations which take some input and, critically, return a single output --- which R reads in the following order:

1.  `Scrape()`
2.  `MakePage()`
3.  `ReadText()`
4.  `CleanText()`
5.  `ReadBook()`
6.  `ReadNo()`
7.  `ReadHeader()`
8.  `CleanHeader()`
9.  `TranslateRomanNumeral()`
10. `TranslateCentury()`
11. `CleanDates()`
12. `ReadDateAfter()`
13. `ReadDateBefore()`
14. `ReadLocation()`
15. `MakeEntry()`

The function `MakePage()` takes a PH number as an input, from which it generates a URL for the page of the number's corresponding inscription on the PHI website. The function then accesses the page through that link and retrieves its contents in a machine-readable format. It then returns these contents, i.e., it makes the contents into a valid variable which it outputs for use by later functions. `MakePage()` calls this output variable `page`.

Next in the chain comes `ReadText()`. This function takes the variable `page` as an input. From this input, the function generates a list of elements called the Document Object Model (DOM). One might best picture the DOM as a tree chart, which arranges every paragraph, button, or other element of a webpage into a series of stems and branches. Every point on this chart --- called a "node" --- has a unique identifier which web browsers use to apply various styles, such as fonts and colors. Since the industry has standardized DOM's, the function `ReadText()` can reproduce this list with full confidence that it will match that used by the PHI website. Considering this, the author has manually retrieved the relevant node identifiers from the site, one of which `ReadText()` uses to identify the main body text of the inscription in the `page`. The function retrieves this inscription body and returns it as the variable `text`.

The function `CleanText()` takes the `text` as an input, then removes any extraneous editorial marks. First, it targets numbers in Arabic numerals, which appear in the text as line numbers and occasionally as indicators for the length of a textual corruption. However, for reasons likely pertaining to storage limitations, the PHI does not maintain dedicated sites for multiple copies of an inscription. This becomes a matter of some consequence when retrieving the data from IG VII, which includes data from the city and environs or Oropos. When digitizing their inscriptional corpus, the PHI included a copy of Vasileios Petrakos' *Hoi Epigraphes Tou Oropou*, which appropriately contains a much more thorough collection of Oropian inscriptions. Therefore, any inscription in the IG which also appears in the Epigraphes does not have a meaningful page among the other IG digitizations. Rather, the entry simply directs the reader to the appropriate Epigraphes entry. As a result, the function `CleanText()` must ignore any numbers occurring in these directions.

Then, after having removed any unnecessary numerals from the text the function deletes anything found between the square brackets \[ \]. As this study relies on extant writings for its analysis, the inclusion of reconstructed text, whatever its quality, would necessarily amount to the inclusion of artificial data which may erroneously sway the results. `CleanText()` follows this by making its final formatting corrections. It removes the angled brackets \< \>, the pound sign #, and any sublinear dots before turning line breaks into single spaces and merging what words had originally split between two lines.

Finally, `CleanText()` makes changes to the characters in the text themselves to both maintain some fidelity to the original inscriptions and simplify later quantitative analyses. As the PHI copies contain only modernized transcriptions of the inscriptions, and not the unaccented 1-to-1 copy as is present in the print editions, some sounds can appear in several orthographic forms. For example, the vowel \textipa{/a/} may take any of any forty-three shapes when accounting for capitalization (miniscule or majuscule), breathing (spiritus asper or spiritus lenis), accent (acute, grave, or circumflex), and historic presence of a diphthong (iota subscript or no iota subscript).

|               |               |               |               |               |               |
|------------|------------|------------|------------|------------|------------|
| \textgreek{α} | \textgreek{ἀ} | \textgreek{ἁ} | \textgreek{Α} | \textgreek{Ἀ} | \textgreek{Ἁ} |
| \textgreek{ά} | \textgreek{ἄ} | \textgreek{ἅ} | \textgreek{Ά} | \textgreek{Ἄ} | \textgreek{Ἅ} |
| \textgreek{ὰ} | \textgreek{ἂ} | \textgreek{ἃ} | \textgreek{Ὰ} | \textgreek{Ἂ} | \textgreek{Ἃ} |
| \textgreek{ᾶ} | \textgreek{ἆ} | \textgreek{ἇ} | ---           | \textgreek{Ἆ} | \textgreek{Ἇ} |
| \textgreek{ᾳ} | \textgreek{ᾀ} | \textgreek{ᾁ} | \textgreek{ᾼ} | \textgreek{ᾈ} | \textgreek{ᾉ} |
| \textgreek{ᾴ} | \textgreek{ᾀ} | \textgreek{ᾅ} | ---           | \textgreek{ᾌ} | \textgreek{ᾍ} |
| \textgreek{ᾲ} | \textgreek{ᾂ} | \textgreek{ᾃ} | ---           | \textgreek{ᾊ} | \textgreek{ᾋ} |
| \textgreek{ᾷ} | \textgreek{ᾆ} | \textgreek{ᾇ} | ---           | \textgreek{ᾎ} | \textgreek{ᾏ} |

: Possible Combinations of Alpha with Polytonic Accents

These numerous glyphs do represent variance in suprasegmental features and historical pronunciation, as well as some contextual info such as changes in speaker or the presence of a proper noun, but they do little to inform the underlying quality of the vowel itself. In addition, the sheer number of glyphs mapped to each vowel would require unnecessarily complex search parameters to extract the same data from accented text as a much simpler search parameter would from an unaccented text. Although modern computer type does support decomposed characters which, for example, would represent \<\textgreek{ᾆ}\> as four individual glyphs placed atop each other --- \<\textgreek{α}\>, \<\textgreek{ι}\>, \<\textgreek{᾽}\>, and \<\textgreek{῀}\> --- most systems would automatically convert them into a single precomposed glyph \<\textgreek{ᾆ}\>, which it reads as one individual character distinct from all others. As a result, searching the data for \<\textgreek{α}\> will only return \<\textgreek{α}\>, not \<\textgreek{ᾆ}\> or \<\textgreek{ἃ}\>, and so on. Any search parameters used to parse an accented text would therefore need to account for every character combination, which quickly becomes excessive when searching for entire words. As a means of circumventing this, `CleanText()` finds these characters once at the very beginning and substitutes them for their unaccented counterparts, though it preserves majuscules and minuscules for ease of reading when verifying search results. Without accents to contend with, R can more easily handle this case distinction. The function applies all these changes to the `text`, which it stores for later use.

Then the web scraper changes focus and begins capturing an inscription's identifying data. While a PH number provides sufficient labelling for a computer to work with, it offers little for the researcher themselves. Therefore, the scraper uses the functions `ReadBook()` and `ReadNumber()` to capture a more useful identifier from the `page` variable. As the names suggest, `ReadBook()` captures which volume of the IG contains an inscription, and `ReadNumber()` captures that inscriptions number within the volume. The scraper saves these as the variables `ig_book` and `ig_no` which it stores for later use.

With the text and identifiers sorted, the scraper can begin the seven-step process of extracting an inscription's approximate date of carving. The multi-decade publishing history of the IG had resulted in an unstandardized patchwork of various dating formats which, despite all presenting adequately the relevant chronological information, most thoroughly confound a computer. Of the seven, five functions serve only to help standardize the carving dates, whereas only two capture the relevant information. These functions are numbers seven through thirteen in the order of operations.

7.  `ReadHeader()`
8.  `CleanHeader()`
9.  `TranslateRomanNumeral()`
10. `TranslateCentury()`
11. `CleanDates()`
12. `ReadDateAfter()`
13. `ReadDateBefore()`

The process relies on a line of text here referred to as the header, which contains descriptions of stoichedon, dates, locations, and cross references as available. As with the other inscriptional elements, the PHI has standardized the position of the header on a page, allowing for the function `ReadHeader()` to extract and store it as the variable `header`. The function `CleanHeader()` takes the `header` as an input and cleans it in a similar way to `CleanText()`. It removes editorial marks -- brackets, pound signs, and so on -- and transforms any line breaks or tab stops into single spaces in order to ease processing later on. `ReadHeader()` also erases stoichedon information, which appears in a similar format to dates and may hinder proper date extraction.

After this initial cleaning, the scraper calls the function `TranslateRomanNumeral()`, which performs two major standardizing operations. First, it converts any centuries written in Roman numerals to Arabic numerals, which R can more readily interpret. Then, the function finds which Latin abbreviations appear in a standardized position, and so constitute a critical element through which the function identifies centuries and translates them into English. For example, the inscription IG I^3^ 401 has in its header the date "s. V a.". The function uses the abbreviation "s." for "saeculum" as a marker to indicate that the following text relays numerical information. It then grabs that text -- "V" -- and converts it into an Arabic numeral, resulting in the string "s. 5 a.". As the following function uses the abbreviation "c." for "century" to find its target strings, it becomes necessary to translate the "saeculum" abbreviation early. To do so, `TranslateRomanNumeral()` takes "s. 5 a." (or any other date of the same format) and converts it to "5th c. a.".

Then, with the centuries standardized, the scraper calls the function `TranslateCentury()`. The IG frequently gives dates as centuries or century ranges which, while plainly evident to a person, causes some difficulty for a computer. Therefore, the function finds centuries and converts them to `DDD-DDD` format. The operations in `TranslateCentury()` are context aware: if either the term "early" or "late" precedes the century, then the function will convert it to a fifty-year date range. As such, a date reading "early 4th c. a." would become "400-350 a."; a date reading "late 4th c. a." would accordingly become "350-301". If the phrase "mid." precedes the century, then the function returns a date in the mid-century. If no phrase precedes it, then the function returns a date range covering the century's full duration.

As a final preparatory step, the function `CleanDates()` performs any remaining standardizations required for accurate data extraction. First, it translates any remaining Latin text into English; then, it reformats date ranges. The IG has used several date-range formats throughout its publishing history, the most relevant here being the "slashed" format `DDD/D` and the "dashed" format `DDD-D`. Both slashed and dashed dates may have one to three digits on the right side. In addition, dashed dates sometimes occur as long chains of dates in `DDD-DDD-DDD-DDD` format, as a consequence of how `TranslateCentury()` handles century ranges. `CleanDates()` works to standardize all of these in dashed format with two dates, `DDD-DDD`, each written with three digits. The function will only make an exception for dates within the first century, to which it will never add leading zeroes.

With the header cleaned and the dates captured, the scraper may gather the final piece of evidence. using the `header` as an input, the function `ReadLocation()` reads the place of discovery as available for an inscription. Whereas this study restricts itself to Athens and her closest neighbors, the editors of the IG had no such limitations in mind, and so their publications assemble by region. Consequently, the data set contains inscriptions from cities not relevant to the study. To account for this, `ReadLocation()` determines an inscription's place of origin for later filtering, which it returns as the variable `location`.

Now the scraper has eight variables: `ig_book`, '`ig_no`', '`phi_no`', '`header`', '`location`', '`date_before`', `date_after`, and `text`. The function `MakeEntry()` takes these variables as inputs and bundles them together as a variable type called a `list`. As the name implies, a `list` contains a series of values (called "components"). This has two major benefits: 1) now that a copy of the variables exist and are stored in a container which permanently associates them with one another, the scraper can reuse the variable names for other inscriptions without fear of loss or confusion; and 2) the components of the list appear in a standardized order, allowing them to more easily integrate with a table of evidence. After generating a list, `MakeEntry()` returns it as the variable `entry`.

The very first function on the list, `Scrape()`, manages the entire process. It takes a PH number as an input, which it stores as the variable `phi_no`. It then manages the processes of scraping, cleaning, and reading. `Scrape()` provides every input to the other functions as it calls them, and it stores their outputs as named variables as they return them.

However, it must do so contextually. While every PH number is unique, and the numbers are sequential within a volume, they are not sequential between volumes. Consecutive volumes of the IG occasionally have large gaps in identifiers between them, and the unassigned PH numbers will crash the scraper if left unaddressed. To mitigate this problem, `Scrape()` will pause operations after `ReadText()` and take a simple measurement of the `text` length by counting the number of characters stored in it. Because the PHI has standardized their page structure so well, inputting an empty PH number into an address and trying to load it will always bring up the same error page. This page has no body text comparable to that of a valid inscription page, and so has no equivalent node identifier on the DOM. When `ReadText()` tries to call an identifier but cannot find one, it returns an empty variable with a length of zero.

This empty variable specifically causes crashes, as the function `ReadText()` --- and any other function using regular expression, that is, any function which needs to perform a search --- expects an input with a length greater than or equal to one. However, one may take a cue from these input requirements and define the validity of a page by the presence of text: no text means no inscription, and so no valid page. With this in mind, `Scrape()` uses the length of the text to determine a PH number's validity. If the logical phrase `if(length(text) > 0)` returns the value `1` --- that is, `TRUE` ---then the scraping may proceed. However, if the expression returns the value `0` --- `FALSE` --- then `Scrape()` assigns the value `NA` to all eight variables and passes them to `MakeEntry()`.

After giving its variables -- whatever their value -- to `MakeEntry()`, `Scrape()` has in its possession the `entry`. It returns this list as an output. `Scrape()` itself is called by an external loop, which passes numbers incrementally to it for use as PH numbers, and which adds this `entry` output into a table of data for testing.

## Data Selection

The data required for the study appear in IG I-III (Attica) and IG VII (Megaris, Oropia, and Boiotia). To capture these, the author manually retrieved the PH number for the final inscription in IG VII (VII 4,269) and directed the scraper to retrieve everything up to and including it. This process returned a table with 147,791 rows, which the author saved as a CSV.

The table needed some tidying. To start, the data included 105,684 invalid PH numbers, the inclusion of which added an unnecessary level of burden on every operation. Once rid of them, the tale contained 42,107 inscriptions.

Then, the author removed any entries not from a volume of the IG. The table included entries from two extraneous books: the *Inscriptions de Délos* (ID) and the *Supplementum Epigraphicum Graecum* (SEG). Without these, the table had within it 23 volumes of the IG. The author filtered the irrelevant inscriptions in three steps. First, using the descriptions provided by the PHI, the author classified the volumes by region. Of the six in the table, the study required only two, reducing the table to 6 volumes. Then, with further reference to the PHI website, the author identified four subregions, with two falling outside the scope of the study. Once the author dropped these, the table contained 19,659 inscriptions from four volumes: IG I^2^, IG I^3^, IG II^2^, and IG VII.

With the appropriate volumes having been selected, the task became to reduce the table down to inscriptions only from within the relevant time span. To facilitate this process, the author began by manually verifying each date extracted by the scraper. The code had functioned reasonably well under most conditions, though a few date formats proved consistently elusive. First, 240 inscriptions included the label "post fin. s. D", which the scraper always read as an "s. D" date. For example, the header of IG II^2^ 5,323 (PH# 7,641) contains the date "post fin. s. IV a.", but the scraper assigned to it the date range of 400-301 BCE, having plainly interpreted the text as indicating a 4th century inscription. In this instance, the author has assigned the TPQ 300 BCE, with no TAQ; and has corrected the other instances accordingly.

The set also included 1,101 inscriptions with dates of the type "med. s. D a.", indicating a possible date range somewhere within the middle of a century. Whereas the function `TranslateCentury()` contained instructions for handling "init." and "fin." dates, the author included no such provisions for "med." dates: consequently, the first two formats returned 50-year date ranges, but the latter returned a single static date for both the TPQ and TAQ. IG II^2^ 1,185 (PH# 3,400), for example, included in its header the date "med. s. IV a.". The scraper, however, returned only the date 350 BCE. To correct for this oversight, the author has generated a date range by creating a TPQ 25 years before and TAQ 25 years after the date output by the scraper, resulting in the estimated date range for this inscription of 375-325 BCE.

Next, the author proceeded to remove inscriptions dated to unused eras. Of the 19,695 inscriptions yet remaining in the set, 7,944 contained in their header chronological information allowing for their categorical removal. To start, 2,464 had a date in the format "s. D p.", indicating inscriptions cut during the common era, while 147 contained the tag "AD". Then, the author found 384 inscriptions attributed to the "aet. Rom.", and 55 to the "Roman period." Having interpreted this period as beginning, in the context of central Greece, with Sulla's sack of Athens in 86 BCE, the author categorized these as 1st century inscriptions. Another 596 held the label "aet. imp.", 213 the label "Roman Imperial period", 58 "aet. Augusti", 74 "aet. Hadriani", and 41 the label "reign of X". These too the author categorized as inscriptions from 1^st^ c. BCE or later. Next, 23 inscriptions had the label "Christian". With the works of Saints Peter and Paul traditionally attributed to between the 1st c. BCE and the 1^st^ c. CE, the author categorized these with the other 1^st^ c. BCE inscriptions. Finally, the set included 775 inscriptions labeled as "s. I a." and 11 as "1st c. BC". Seeing as the study focuses on those inscriptions found between the 4th and 2nd centuries BCE, the author categorically removed all later inscriptions. Along with these, the author removed 3,112 inscriptions labeled as "undated". The resulting table had 11,715 inscriptions, the dates for which the author manually validated.

After validating, the author proceeded to select for data within the target 4^th^ -- 2^nd^ c. BCE date range, however, the presence of cross-century date ranges complicated the process. Some have a relatively short range, like IG VII 2,534 (PH# 146,036), cut somewhere between 350 and 201 BCE, whereas others, such as IG VII 2,839 (PH# 146,353), from somewhere between 323 and 31 BCE, could have originated from any century in the target range or even shortly after. This raises the question of how to classify these inscriptions for analysis.

To solve this problem, the author used a simple two-step process. First, they reduced the set to any inscriptions with either a TPQ or TAQ within the 4^th^ -- 2^nd^ c. BCE date range. This involved filtering by three columns created manually by the author during the validation stage: `tpq_cen`, `taq_cen`, and `tpq_bce`. The columns `tpq_cen` and `taq_cen` record the century within which the TPQ and TAQ fall, derived by rounding the appropriate date up to the nearest hundred. 400, then, stands in for the 4^th^ c., 300 for the 3^rd^, and so on. The column `tpq_bce` (and the accompanying `taq_bce`) contains a simple `TRUE` or `FALSE`, recording whether the associated date falls before or within the common era. To perform the selection, the author first selected columns with either a TPQ or a TAQ within the selected range, and then further reduced this set by removing any non-BCE dates where `tpq_bce` equaled `FALSE`, returning a list of 7,904 inscriptions. 1,118 of the removed rows had no date at all, 1,656 fell between the 6^th^ and 5^th^ c. BCE, 7 between the 7^th^ and 8^th^ c., and the remainder fell within or after the 1^st^ c. CE .

As the second step, the author the author assigned a final century of analysis to each inscription. For the tautocenturial inscriptions, the process posed little difficulty: IG I^2^ 561 (PH# 1,760), for example, dated between 400 and 301 BCE, should be analyzed as a 4^th^ c. BCE inscription. However, the matter becomes somewhat more complicated when dealing with inscriptions the likes of IG I^3^ 1,057 (PH# 1,198), which dated to somewhere between 500 and 301 BCE. With no further guidance from the PHI or the IG, the author interpreted this range as one assigning to each year therein an equiprobable chance of being the inscription's date of origin. To accommodate the study's century-by-century analysis, the author calculated the floored mean of each inscription's TPQ and TAQ, then assigned the inscriptions to that mean's century by rounding up to the nearest hundred in the exact same manner as with the columns `tpq_cen` and `taq_cen`. The author stored these results in a new column called `analysis_cen`. For IG I^3^ 1,057, the mean of the TPQ and TAQ is 400.5, which floors to 400 BCE. As such, the author has categorized the inscription as one from the 4^th^ c. BCE. After assigning each inscription to its century of analysis in this manner, the author performed the final chronological selection, removing every inscription with an `analysis_cen` outside of the target range. The resulting table contained 7,364 inscriptions.

After finishing the chronological selection, the author then selected for location. To start, they manually validated the locations extracted by `ReadLocation()`, of which the table contained 26. As the study only targets seven city centers -- Athens, Eleusis, Megara, Pagai, Aigosthena, Oropos, and Tanagra -- the author removed all inscriptions categorized elsewhere, 361 in total. Unfortunately, the volume IG II^2^ contained 6,276 inscriptions with no location of origin besides "Att(ica)", and IG I^2^ another 8. Insofar as the study requires each inscription to correlate with a city, a limitation intended to help when creating isogloss maps, these inscriptions offered no practical use, and so the author removed them. The resulting table included 716 rows of inscriptions. After one final cleaning to remove any Roman characters or Arabic numerals, the author unnested the inscriptions --- that is, assigned each word to its own row, while retaining all other information about volume, chronology, location, and so on. The final data subset then contained 47,485 distinct tokens for analysis.

```{r Counting_Inscriptions}
#| label: tbl-inscription-counts
#| tbl-cap: Number of Inscriptions by City
count(NestedData, location)
```

## Testing

To test for linguistic variation between cities, the author used Fisher's Exact test to compare the proportion of occurrences of "Attic" tokens (meaning those tokens from Athens, and all matching non-Athenian tokens) and of "non-Attic" tokens counted between Athens and another target city within a given century. In order to maintain as many phonological conditions as possible during a test, the author selected a list of common, representative words from the inscriptions in Eduard Schwyzer's *Dialectorum Graecarum Exempla Epigraphica Potiora* (DGE). These words were then compared against their common, Attic-Ionic equivalent, and both forms were collected as tokens for testing.

The author chose Fisher's Exact test over the Chi-Squared test to account for the often fragmentary nature of the inscriptions and the disparity between the number of inscriptions from each city. Athens and Eleusis, taken together, have more inscriptions than all other locations combined in this data set; with the same pattern arising for analyzable tokens. For comparison, Pagai has only two inscriptions and 244 tokens. Besides this, many tokens proved unusable given the amount of fragmentation suffered. These factors together had raised the likelihood that fewer than 10 observations of a token may represent a city for any given trial, which would have invalidated the results of a Chi-Squared test. However, Fisher's Exact test, designed specifically for smaller pools of test data but similarly valid for larger sets, had no such limitation.

Yet it suffers one downside when compared to the Chi-Squared test. Namely, one cannot perform the basic test on tables of proportions greater than 2x2. That is, the test results would be invalid had the author performed it on a sample including three or more locations, or one including three or more tokens. The author, then, could not compare the observations across all locations at once, nor could they compare multiple dialectal forms at once. However, since the R programming language includes built-in functions for performing Fisher's Exact test, any negative consequences of these testing limitations -- namely, the need to perform multiple tests in succession -- are negated.

The test returns a single number called the *p*-value. This number is compared against an alpha value, which traditionally is, and in this study has been, operationalized as 0.05. The *p*-value represents the estimated uncertainty in a result by indicating the likelihood that any differences in the data result of random chance. A *p*-value of 0.07 indicates a 7% likelihood that the differences are random, whereas a *p*-value of 0.03 indicates only a 3% likelihood. The alpha value of 0.05 indicates that the author would accept a maximum 5% chance that the variations observed in the data were random.
